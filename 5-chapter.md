## 式5.2（感知学习参数更新）
$$\Delta w_i=\eta(y-\hat{y})x_i$$
给出的公式是在线性回归或神经网络权重更新中使用的梯度下降算法的基本形式。这个公式用于根据当前的预测误差调整权重 $w_i$。

公式中的符号含义如下：

- $\Delta w_i$：权重 $w_i$ 的变化量。
- $\eta$：学习率（Learning Rate），一个超参数，用于控制权重更新的步长。
- $y$：真实目标值。
- $\hat{y}$：模型预测值。
- $x_i$：输入特征值，与权重 $w_i$ 相乘的值。

梯度下降算法的基本思想是最小化预测误差，即 $y$ 和 $\hat{y}$ 之间的差异。这个差异通常称为损失函数（Loss Function），可以是均方误差（Mean Squared Error, MSE）或其他度量预测误差的函数。

权重更新的公式是：

$$\Delta w_i = \eta(y - \hat{y})x_i$$

这个公式的含义是：

1. 首先计算预测误差，即 $y - \hat{y}$。
2. 将预测误差乘以学习率 $\eta$，得到一个标量乘积。
3. 将这个标量乘积乘以特征值 $x_i$，得到权重 $w_i$ 的变化量 $\Delta w_i$。

然后，使用这个变化量来更新权重：

$$w_i := w_i + \Delta w_i$$

这个过程在每次迭代中重复执行，直到满足停止条件，例如损失函数的值不再显著减小，或者达到预设的迭代次数。

梯度下降算法是机器学习中用于优化问题的一种常用方法，特别是在训练线性模型和神经网络时。通过适当选择学习率和其他超参数，梯度下降算法可以帮助我们找到损失函数的最小值，从而训练出性能良好的模型。

## 式5.10 (反向传播1）
$$\begin{aligned}
g_j&=-\frac{\partial {E_k}}{\partial{\hat{y}_j^k}} \cdot \frac{\partial{\hat{y}_j^k}}{\partial{\beta_j}}
\\&=-( \hat{y}_j^k-y_j^k ) f ^{\prime} (\beta_j-\theta_j)
\\&=\hat{y}_j^k(1-\hat{y}_j^k)(y_j^k-\hat{y}_j^k)
\end{aligned}$$

给出的公式是神经网络中反向传播算法的一部分，用于计算损失函数 $E_k$ 关于第 $j$ 个神经元第 $k$ 层输出 $\hat{y}_j^k$ 的梯度 $g_j$。这个梯度是权重 $\beta_j$ 更新的关键部分。

公式中的符号含义如下：

- $g_j$：第 $j$ 个神经元在第 $k$ 层的梯度。
- $E_k$：第 $k$ 层的损失函数。
- $\hat{y}_j^k$：第 $j$ 个神经元在第 $k$ 层的预测输出。
- $y_j^k$：第 $j$ 个神经元在第 $k$ 层的真实输出或目标值。
- $\beta_j$：第 $j$ 个神经元的输入加权求和（不包括偏置项）。
- $\theta_j$：第 $j$ 个神经元的阈值或偏置。
- $f'(\beta_j - \theta_j)$：激活函数 $f$ 对输入 $\beta_j - \theta_j$ 的导数。

梯度计算的步骤如下：

1. 计算损失函数 $E_k$ 关于预测输出 $\hat{y}_j^k$ 的偏导数，这是损失对输出的敏感度。

$$\frac{\partial E_k}{\partial \hat{y}_j^k} = -(\hat{y}_j^k - y_j^k)$$

2. 计算预测输出 $\hat{y}_j^k$ 关于权重 $\beta_j$ 的偏导数，这通常涉及激活函数 $f$ 的导数。

$$\frac{\partial \hat{y}_j^k}{\partial \beta_j} = f'(\beta_j - \theta_j)$$

3. 将两个偏导数相乘，得到损失函数 $E_k$ 关于权重 $\beta_j$ 的梯度。

$$g_j = -\frac{\partial E_k}{\partial \hat{y}_j^k} \cdot \frac{\partial \hat{y}_j^k}{\partial \beta_j}$$

4. 对于 Sigmoid 激活函数，其导数 $f'(x)$ 是 $\hat{y}(1 - \hat{y})$。如果 $\hat{y}_j^k$ 是 Sigmoid 函数的输出，那么 $f'(\beta_j - \theta_j) = \hat{y}_j^k(1 - \hat{y}_j^k)$。

5. 代入激活函数的导数，得到梯度的最终表达式：

$$g_j = \hat{y}_j^k(1 - \hat{y}_j^k)(y_j^k - \hat{y}_j^k)$$

这个梯度 $g_j$ 表示了在训练过程中如何调整权重 $\beta_j$ 以减少损失函数 $E_k$ 的值。在实际应用中，这个梯度将用于更新权重，通过梯度下降或其变体进行优化。

## 式5.12（反向传播2）
$$\Delta \theta_j = -\eta g_j$$
给出的公式是神经网络中反向传播算法的一部分，用于更新第 $j$ 个神经元的偏置项 $\theta_j$。这个更新过程是梯度下降算法的应用，目的是通过最小化损失函数来优化神经网络的参数。

公式中的符号含义如下：

- $\Delta \theta_j$：第 $j$ 个神经元偏置项的更新量。
- $\eta$：学习率（Learning Rate），一个超参数，用于控制更新步长的大小。
- $g_j$：第 $j$ 个神经元的梯度，通常计算为损失函数关于该神经元输出的导数。

偏置项更新的公式是：

\$$\Delta \theta_j = -\eta g_j$$

这个公式的含义是：

1. 计算第 $j$ 个神经元的梯度 $g_j$，它反映了损失函数对神经元输出的敏感度。
2. 梯度 $g_j$ 乘以学习率 $\eta$，得到更新量 $\Delta \theta_j$。
3. 更新量与当前偏置值相减，更新偏置项：

\$$\theta_j := \theta_j + \Delta \theta_j$$

在神经网络的训练过程中，每次迭代都会进行这样的参数更新，直到满足某个停止条件，如损失函数值下降到一个很小的值，或者达到预设的最大迭代次数。

梯度 $g_j$ 的具体计算方法依赖于所使用的激活函数和损失函数。例如，在使用交叉熵损失函数和 Sigmoid 激活函数的情况下，梯度可以表示为：

\$$g_j = \hat{y}_j(1 - \hat{y}_j)(y_j - \hat{y}_j)$$

这里，$\hat{y}_j$ 是神经元的预测输出，$y_j$ 是真实目标值。这个梯度反映了预测输出与目标值之间的差异，用于指导偏置项的更新。

## 式5.13（反向传播3）
$$\Delta v_{ih} = \eta e_h x_i$$
给出的公式是神经网络中反向传播算法的一部分，用于更新从输入层到隐藏层的权重 $v_{ih}$。这个更新过程同样基于梯度下降算法，目的是通过减少网络的总体误差来优化权重。

公式中的符号含义如下：

- $\Delta v_{ih}$：从输入单元 $i$ 到隐藏单元 $h$ 的权重的更新量。
- $\eta$：学习率（Learning Rate），一个超参数，用于控制权重更新的步长。
- $e_h$：隐藏单元 $h$ 的误差项，通常计算为损失函数关于隐藏层输出的导数乘以隐藏层激活函数的导数。
- $x_i$：输入层单元 $i$ 的激活值或输入特征值。

权重更新的公式是：

$$\Delta v_{ih} = \eta e_h x_i$$

这个公式的含义是：

1. 计算隐藏单元 $h$ 的误差项 $e_h$，这通常是后向传播过程中计算得到的，反映了损失函数对隐藏层输出的敏感度。
2. 将误差项 $e_h$ 乘以学习率 $\eta$，得到一个标量乘积。
3. 将这个标量乘积乘以输入值 $x_i$，得到权重 $v_{ih}$ 的更新量 $\Delta v_{ih}$。
4. 更新权重：

$$v_{ih} := v_{ih} + \Delta v_{ih}$$

在神经网络的训练过程中，这样的权重更新会在每次迭代中进行，目的是通过梯度下降或其变体来最小化损失函数。

误差项 $e_h$ 的具体计算方法依赖于网络的架构和所使用的损失函数。在多层网络中，隐藏层的误差项通常由更高层（如输出层）的误差项通过链式法则逐层传递计算得到。这个过程涉及到对每个层的激活函数求导，并将这些导数相乘以传播误差。

## 式5.14（反向传播4）
$$\Delta \gamma_h= -\eta e_h$$
给出的公式是神经网络中反向传播算法的一部分，用于更新第 $h$ 个隐藏单元的偏置项 $\gamma_h$。这个更新过程旨在通过减少网络的总体误差来优化隐藏层的偏置参数。

公式中的符号含义如下：

- $\Delta \gamma_h$：第 $h$ 个隐藏单元偏置项的更新量。
- $\eta$：学习率（Learning Rate），一个超参数，用于控制更新步长的大小。
- $e_h$：第 $h$ 个隐藏单元的误差项，通常计算为损失函数关于隐藏层输出的导数乘以隐藏层激活函数的导数。

偏置项更新的公式是：

$$\Delta \gamma_h = -\eta e_h$$

这个公式的含义是：

1. 计算第 $h$ 个隐藏单元的误差项 $e_h$，它反映了损失函数对隐藏层输出的敏感度。
2. 将误差项 $e_h$ 乘以学习率 $\eta$ 的负值，得到更新量 $\Delta \gamma_h$。
3. 更新隐藏单元的偏置项：

$$\gamma_h := \gamma_h + \Delta \gamma_h$$

在神经网络的训练过程中，每次迭代都会进行这样的参数更新，目的是通过梯度下降或其变体来最小化损失函数。

误差项 $e_h$ 的具体计算方法依赖于网络的架构和所使用的损失函数。在多层网络中，隐藏层的误差项通常由更高层（如输出层）的误差项通过链式法则逐层传递计算得到。这个过程涉及到对每个层的激活函数求导，并将这些导数相乘以传播误差。通过这种方式，网络可以学习如何调整其隐藏层的偏置项，以改善整体的预测性能。

