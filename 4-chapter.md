## 式4.1(熵）
$$\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_{2}{p_k}$$
给出的公式是熵（Entropy）的定义，用于信息论和概率论中，衡量一个随机变量的不确定性或信息量。在机器学习中，熵常用于评估分类问题中的数据集纯度。

公式中的符号含义如下：

- $\operatorname{Ent}(D)$：数据集 $D$ 的熵。
- $|\mathcal{Y}|$：数据集中可能结果（类别）的总数。
- $pk$：数据集中第 $k$ 个结果发生的概率。
- $\log_{2}$：以 2 为底的对数，用于计算信息量，单位是比特（bit）。

熵的计算公式是：

$$
\operatorname{Ent}(D) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_{2} p_k
$$

这个公式的含义是，对于数据集中的每个可能结果，计算其发生概率的以 2 为底的对数，然后乘以该概率（概率的对数是负数，因为概率小于 1），最后对所有可能的结果求和。

熵的特点：

- 当数据集完全随机，即所有类别出现的概率都相等时，熵达到最大值。如果有 $n$ 个类别，最大熵是 $\log_{2} n$。
- 当数据集中所有样本都属于同一个类别时，熵为 0，表示没有不确定性。
- 熵是对称的，即数据集的熵不会因为类别的重新排序而改变。

在机器学习中，熵可以用于：

- 决策树的构建：在选择分裂属性时，熵可以用来衡量分裂后子节点的纯度，选择使得熵最小（即纯度最高）的属性进行分裂。
- 评估模型的性能：较低的熵通常表示模型预测的不确定性较低。

熵也可以推广到条件熵（Conditional Entropy），用于衡量在已知某些条件下，另一个随机变量的不确定性。

## 式4.2（信息增益）
$$\operatorname{Gain}(D,a) = \operatorname{Ent}(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\operatorname{Ent}({D^v})$$

给出的公式是信息增益（Information Gain）的计算方法，它是决策树算法中用于选择最佳分裂属性的关键指标。信息增益衡量了属性 $a$ 对数据集 $D$ 进行分裂后，数据集纯度的提升程度。

公式中的符号含义如下：

- $\operatorname{Gain}(D, a)$：属性 $a$ 对数据集 $D$ 的信息增益。
- $\operatorname{Ent}(D)$：数据集 $D$ 的熵，表示数据集的初始纯度。
- $V$：属性 $a$ 可能的值的个数。
- $D^v$：数据集 $D$ 中，属性 $a$ 取值为 $v$ 的所有记录组成的子集。
- $|D^v|$：子集 $D^v$ 中记录的数量。
- $|D|$：原始数据集 $D$ 中记录的总数。
- $\operatorname{Ent}(D^v)$：子集 $D^v$ 的熵。

信息增益的计算公式是：

$$
\operatorname{Gain}(D, a) = \operatorname{Ent}(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} \operatorname{Ent}(D^v)
$$

这个公式的含义是，首先计算原始数据集 $D$ 的熵，然后计算每个可能的值 $v$ 对应的子集 $D^v$ 的熵，并将这个熵乘以该子集占原始数据集的比例。最后，将原始数据集的熵减去所有子集熵的加权和，得到信息增益。

信息增益的特点：

- 如果分裂后的某个子集是纯净的（即所有记录属于同一个类别），那么这个子集的熵为 0，对信息增益没有贡献。
- 如果分裂后的子集与原始数据集的熵相同，那么这个属性 $a$ 不提供任何信息增益。
- 信息增益倾向于选择具有更多值的属性，因为更多的子集可能意味着更高的纯度提升。

在决策树算法中，通常会计算每个属性的信息增益，并选择信息增益最大的属性作为节点的分裂依据。

## 式4.6（基尼系数）
$$\operatorname{Gini\_index}(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}\operatorname{Gini}(D^v)$$
给出的公式是基尼指数（Gini Index）在决策树算法中的计算方法，用于评估属性 $a$ 对数据集 $D$ 进行分裂后各个子集的不纯度。

公式中的符号含义如下：

- $\operatorname{Gini\_index}(D, a)$：属性 $a$ 对数据集 $D$ 的基尼指数，用于衡量分裂后数据集的不纯度。
- $V$：属性 $a$ 可能的值的个数。
- $D^v$：数据集 $D$ 中，属性 $a$ 取值为 $v$ 的所有记录组成的子集。
- $|D^v|$：子集 $D^v$ 中记录的数量。
- $|D|$：原始数据集 $D$ 中记录的总数。
- $\operatorname{Gini}(D^v)$：子集 $D^v$ 的基尼指数。

基尼指数的计算公式是：

$$
\operatorname{Gini\_index}(D, a) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} \operatorname{Gini}(D^v)
$$

这个公式的含义是，对于数据集中的每个可能的属性值 $v$，计算对应的子集 $D^v$ 的基尼指数，然后将这个基尼指数乘以该子集占原始数据集的比例。最后，将所有子集的加权基尼指数求和，得到整个数据集关于属性 $a$ 的基尼指数。

基尼指数 $\operatorname{Gini}(D^v)$ 的计算方法为：

$$
\operatorname{Gini}(D) = 1 - \sum_{i=1}^{|\mathcal{Y}|} p_i^2
$$

其中，$p_i$ 是数据集 $D$ 中第 $i$ 个类别的概率，$|\mathcal{Y}|$ 是类别的总数。

基尼指数的特点：

- 基尼指数的值介于 0 到 1 之间。值越低表示数据集的纯度越高，即数据集中的记录都属于同一个类别。
- 当数据集完全由单一类别组成时，基尼指数为 0。
- 当数据集中的记录均匀分布在多个类别中时，基尼指数接近 1。

在决策树算法中，基尼指数用于选择最佳的分裂属性，类似于信息增益。但是，基尼指数对子集大小的权重是固定的，而信息增益对较大的子集赋予了更高的权重。因此，基尼指数可能更倾向于选择具有较少值的属性，从而避免过拟合。

## 式4.7（一个集合）
$$T_a={\lbrace{\frac{a^i+a^{i+1}}{2}|1\leq{i}\leq{n-1}}\rbrace}$$
给出的公式定义了一个集合 $T_a$，它包含的是两个连续幂次 $a^i$ 和 $a^{i+1}$ 的平均值，其中 $i$ 从 1 到 $n-1$（包括 1 和 $n-1$）。这里的 $a$ 是底数，$n$ 是考虑的幂次数。

具体来说：

- $a$：底数，可以是任何实数或复数，具体取决于上下文。
- $i$：索引，从 1 开始到 $n-1$ 结束。
- $n$：集合中考虑的幂次数，$T_a$ 中的元素数量将是 $n-1$。

集合 $T_a$ 的数学表达式为：

$$T_a = \left\{ \frac{a^i + a^{i+1}}{2} \mid 1 \leq i \leq n-1 \right\}$$

这意味着集合 $T_a$ 包含 $n-1$ 个元素，每个元素都是对应索引 $i$ 和 $i+1$ 的幂次 $a^i$ 和 $a^{i+1}$ 的平均值。

例如，如果 $a = 2$ 且 $n = 4$，那么集合 $T_a$ 将包含以下元素：

$$T_a = \left\{ \frac{2^1 + 2^2}{2}, \frac{2^2 + 2^3}{2} \right\} = \left\{ \frac{2 + 4}{2}, \frac{4 + 8}{2} \right\} = \left\{ 3, 6 \right\}$$

这个集合可能在不同的数学或计算上下文中有特定的应用，例如在数值分析、几何级数的计算，或者在寻找函数的特定性质时。
## 式4.8(信息增益离散化后的连续属性版本）
$$\begin{aligned}
\operatorname{Gain}(D,a) &= \max_{t\in{T_a}}\operatorname{Gain}(D,a,t)\\
&=
\max_{t\in{T_a}}\operatorname{Ent}(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_t^{\lambda}|}
{|D|}\operatorname{Ent}(D_t^{\lambda})
\end{aligned}$$
给出的公式描述的是使用连续属性进行决策树分裂时的信息增益计算方法。在处理连续属性时，我们需要选择一个阈值 $t$ 来将数据分成两部分。这里的 $T_a$ 是可能的阈值集合，而 $\operatorname{Gain}(D, a, t)$ 是在阈值 $t$ 下由属性 $a$ 对数据集 $D$ 进行分裂的信息增益。

公式中的符号含义如下：

- $\operatorname{Gain}(D, a)$：属性 $a$ 对数据集 $D$ 的信息增益。
- $T_a$：属性 $a$ 的阈值集合，通常是基于数据集 $D$ 中属性 $a$ 的值来确定的。
- $\operatorname{Gain}(D, a, t)$：在阈值 $t$ 下，属性 $a$ 对数据集 $D$ 进行分裂的信息增益。
- $\operatorname{Ent}(D)$：数据集 $D$ 的熵。
- $D_t^{\lambda}$：在阈值 $t$ 下，属性 $a$ 的值 $\lambda$（可以是 $-$ 表示小于 $t$，或 $+$ 表示大于等于 $t$）对应的子集。
- $|D_t^{\lambda}|$：子集 $D_t^{\lambda}$ 中记录的数量。
- $|D|$：原始数据集 $D$ 中记录的总数。

信息增益的计算公式是：

$$
\operatorname{Gain}(D, a) = \max_{t \in T_a} \left( \operatorname{Ent}(D) - \sum_{\lambda \in \{-, +\}} \frac{|D_t^{\lambda}|}{|D|} \operatorname{Ent}(D_t^{\lambda}) \right)
$$

这个公式的含义是，首先计算原始数据集 $D$ 的熵，然后对于每个可能的阈值 $t$，计算分裂后的两个子集的熵，并将这些熵乘以相应的子集占原始数据集的比例。最后，从原始数据集的熵中减去分裂后子集熵的加权和，得到在该阈值下的信息增益。在所有可能的阈值 $t$ 中，选择使得信息增益最大的阈值 $t$。

在实际应用中，这个过程通常涉及以下步骤：

1. 对于连续属性 $a$，确定可能的阈值集合 $T_a$，这可能包括数据集中该属性的所有值。
2. 对于每个阈值 $t$，计算分裂后两个子集的熵。
3. 使用上述公式计算每个阈值的信息增益。
4. 选择信息增益最大的阈值作为分裂点。

这种方法可以帮助决策树算法在选择分裂属性时，有效地处理连续属性。

## 附1 （互信息）
$$\operatorname{I}(Y;X) = \operatorname{Ent}(Y) - \operatorname{Ent}(Y|X)$$

给出的公式是互信息（Mutual Information, MI）的定义，用于度量两个随机变量 $X$ 和 $Y$ 之间的相互依赖性。互信息可以告诉我们 $X$ 的知识如何减少对 $Y$ 的不确定性，反之亦然。

公式中的符号含义如下：

- $\operatorname{I}(Y;X)$：随机变量 $X$ 和 $Y$ 之间的互信息。
- $\operatorname{Ent}(Y)$：随机变量 $Y$ 的熵，表示 $Y$ 的不确定性。
- $\operatorname{Ent}(Y|X)$：条件熵，表示在已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。

互信息的计算公式是：

$$
\operatorname{I}(Y;X) = \operatorname{Ent}(Y) - \operatorname{Ent}(Y|X)
$$

这个公式的含义是，互信息等于 $Y$ 的熵减去 $Y$ 在给定 $X$ 条件下的条件熵。如果 $X$ 和 $Y$ 是独立的，那么知道 $X$ 的信息不会对 $Y$ 产生任何影响，此时互信息为 0。如果 $X$ 包含关于 $Y$ 的信息，那么互信息将是一个正值，表示 $X$ 的知识可以减少对 $Y$ 的不确定性。

互信息的特点：

- 互信息是非负的，即 $\operatorname{I}(Y;X) \geq 0$。
- 当 $X$ 和 $Y$ 完全相关时，互信息达到最大值，等于任一变量的熵。
- 当 $X$ 和 $Y$ 独立时，互信息为 0。

在机器学习中，互信息可以用于特征选择，帮助识别最有信息量的特征，从而提高模型的性能。此外，互信息也广泛应用于信息论、信号处理、机器学习等领域。

## 附2.1
$$v^* = \arg\min_{v}\left[\min_{c_1}\sum_{\boldsymbol {x}_i\in{R_1(a,v)}}{(y_i - c_1)}^2 + \min_{c_2}\sum_{\boldsymbol {x}_i\in{R_2(a,v)}}{(y_i - c_2)}^2 \right]$$
给出的公式用于寻找最优的分割值 $v^*$，通常用于二维数据的线性分割问题，例如在支持向量机（SVM）或其他分类算法中。这个公式试图找到一个分割点 $v$，使得数据可以被分为两个区域 $R_1(a, v)$ 和 $R_2(a, v)$，这两个区域分别对应于不同的类别或响应变量 $y$。

公式中的符号含义如下：

- $v^*$：最优的分割值。
- $\arg\min_{v}$：表示寻找使得代价函数最小的 $v$ 的值。
- $c_1$ 和 $c_2$：两个区域的最优拟合参数（例如，线性回归中的斜率和截距）。
- $\boldsymbol{x}_i$：数据点的特征向量。
- $y_i$：第 $i$ 个数据点的目标值。
- $R_1(a, v)$ 和 $R_2(a, v)$：由属性 $a$ 和分割值 $v$ 确定的两个区域。

代价函数是：
$$v^* = \arg\min_{v}\left[\min_{c_1}\sum_{\boldsymbol {x}_i\in{R_1(a,v)}}{(y_i - c_1)}^2 + \min_{c_2}\sum_{\boldsymbol {x}_i\in{R_2(a,v)}}{(y_i - c_2)}^2 \right]$$

这个公式的含义是：

1. 对于每个可能的分割值 $v$，我们计算两个区域 $R_1$ 和 $R_2$。
2. 在每个区域内，我们寻找最佳拟合参数 $c_1$ 和 $c_2$，使得该区域内的预测误差（残差平方和）最小。
3. 我们计算在分割值 $v$ 下，整个数据集的总预测误差（两个区域的残差平方和）。
4. 最后，我们寻找使得总预测误差最小的分割值 $v^*$。

在实际应用中，这个过程可以通过以下步骤实现：

1. 遍历属性 $a$ 的所有可能值作为分割点 $v$。
2. 对于每个分割点，计算两个区域并分别进行最小二乘拟合。
3. 计算每个分割点的残差平方和。
4. 选择具有最小残差平方和的分割点 $v^*$。

这种方法通常用于寻找数据的最佳分割，以便在分类或回归问题中实现更好的性能。

## 附2.2（CART回归树）
$$f(\boldsymbol {x}) = \sum_{m=1}^{M}c_m\mathbb{I}(x\in{R_m})$$

给出的公式表示的是一个分段函数 $f(\boldsymbol{x})$，通常在机器学习中的一些特定算法（如支持向量机）中使用，用于表示基于不同区域 $R_m$ 的分类决策或回归预测。

公式中的符号含义如下：

- $f(\boldsymbol{x})$：定义在输入向量 $\boldsymbol{x}$ 上的函数。
- $M$：区域的总数。
- $c_m$：当输入 $x$ 属于区域 $R_m$ 时，函数 $f$ 所取的常数值。
- $\mathbb{I}(x \in R_m)$：指示函数（Indicator Function），如果 $x$ 属于区域 $R_m$，则该函数值为 1；否则为 0。
- $R_m$：定义域中的第 $m$ 个区域。

这个函数的计算方法如下：

1. 对于每一个区域 $R_m$，检查输入向量 $\boldsymbol{x}$ 是否属于该区域。
2. 如果 $\boldsymbol{x}$ 属于区域 $R_m$，则 $\mathbb{I}(x \in R_m)$ 的值为 1，否则为 0。
3. 函数 $f$ 的值是所有 $c_m$ 与对应的指示函数 $\mathbb{I}(x \in R_m)$ 乘积的和，即：

$$f(\boldsymbol{x}) = \sum_{m=1}^{M} c_m \mathbb{I}(x \in R_m)$$

这意味着 $f(\boldsymbol{x})$ 的最终输出将是 $\boldsymbol{x}$ 所在的区域对应的 $c_m$ 值。

在机器学习中，这种函数可以用于：

- 分段线性或非线性模型的预测。
- 决策函数，其中不同的区域对应于不同的类别或决策结果。
- 支持向量机中的决策边界，其中 $R_m$ 可以是支持向量机中定义的间隔区域。

分段函数是一种非常灵活的模型，因为它可以通过调整区域和对应的常数来适应复杂的数据模式。

